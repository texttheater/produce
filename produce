#!/usr/bin/env python3

import argparse
import ast
import errno
import logging
import os
import re
import shlex
import shutil
import subprocess
import sys
import time

from collections import defaultdict, OrderedDict
from contextlib import ExitStack
from tempfile import NamedTemporaryFile
from threading import BoundedSemaphore, RLock, Thread

### PLANNED FEATURES

# TODO change log output to be both friendlier to the human eye and more
# machine-readable. Then write unit tests that check for not doing the same
# thing twice, e.g. when two targets are outputs of the same IRule or when a
# recipe fails for a target that multiple other targets depend on. Then,
# finally, merge feat/outputs into master.
# TODO includes
# TODO variation on the file rule type that unconditionally runs the recipe
# but with a temp file with target, then percolates dirtiness only if the new
# file differs from the existing version - e.g. for files that depend on data
# from a database.
# TODO flat producing: run the recipe of the target, touch all existing
# dependencies
# TODO optionally delete intermediate files
# TODO rename private members

### UTILITIES

def have_smart_terminal():
    # courtesy of apenwarr's redo
    return (os.environ.get('TERM') or 'dumb') != 'dumb'

def dedup(seq):
    """
    Deduplicates a sequence - courtesy of
    http://www.peterbe.com/plog/uniqifiers-benchmark
    """
    seen = set()
    seen_add = seen.add
    return [x for x in seq if not x in seen and not seen_add(x)]

def mtime(path, default=0):
    """
    Returns the mtime of the file at the specified path. Returns default if
    file does not exist. An OSError is raised if the file cannot be stat'd.
    """
    try:
        return os.stat(path).st_mtime
    except OSError as e:
        if e.errno == errno.ENOENT:
            return default
        raise e

def now():
    return time.time()

def remove_if_exists(path):
    try:
        os.remove(path)
    except OSError as e:
        if e.errno == errno.EISDIR:
            shutil.rmtree(path)
        elif e.errno == errno.ENOENT:
            pass # Mission. F***ing. Accomplished.
        else:
            raise e

def rename_if_exists(src, dst):
    try:
        os.rename(src, dst)
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise e

def shlex_join(value):
    return ' '.join((shlex.quote(str(x)) for x in value))

### CUSTOMIZED LOGGING
# It's a pain in the neck in Python.

class TargetLogMessage():

    def __init__(self, action, target, depth):
        self.action = action
        self.target = target
        self.depth = depth

    def __str__(self):
        return '{} {}'.format(self.action, self.target)

class TargetLogFormatter(logging.Formatter):

    def __init__(self, colors):
        logging.Formatter.__init__(self)
        if colors:
            self.red    = '\x1b[31m'
            self.green  = '\x1b[32m'
            self.yellow = '\x1b[33m'
            self.bold   = '\x1b[1m'
            self.plain  = '\x1b[m'
        else:
            self.red = ''
            self.green = ''
            self.yelloe = ''
            self.bold = ''
            self.plain = ''

    def format(self, record):
        if isinstance(record.msg, TargetLogMessage):
            return '{}{}{}{}{}{}{}'.format(self.green, record.msg.action,
                    ' ' * (16 - len(record.msg.action)), self.bold,
                    ' ' * record.msg.depth, record.msg.target, self.plain)
        else:
            return logging.Formatter.format(self, record)

logger = logging.getLogger('produce')
logger.propagate = False
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
handler.setLevel(logging.INFO)
formatter = TargetLogFormatter(sys.stderr.isatty() and have_smart_terminal())
handler.setFormatter(formatter)
logger.addHandler(handler)

### ERRORS

class ProduceError(Exception):

    def __init__(self, message, cause=None):
        Exception.__init__(self, message)
        self.cause = cause

### COMMANDLINE PROCESSING

def process_commandline(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument('-B', '--always-build', action='store_true',
            help="""unconditionally build all targets""")
    parser.add_argument('-d', '--debug', action='store_true', help="""print
            debugging information""")
    parser.add_argument('-f', '--file', default='produce.ini',
            help="""use FILE as a Producefile""")
    parser.add_argument('-j', '--jobs', type=int, default=1, help="""Specifies
            the number of jobs (recipes) to run simultaneously.""")
    parser.add_argument('-n', '--dry-run', action='store_true', help="""print
            the commands that would be executed, but do not execute them""")
    parser.add_argument('-s', '--silent', action='store_true', help="""don't
            print the commands that are executed""")
    parser.add_argument('-u', '--pretend-up-to-date', metavar='FILE',
            action='append', default=[],
            help="""do not rebuild FILE or its dependencies (unless they are
            also depended on by other targets) even if out of date, but make
            sure that future invocations of Produce will still treat them as
            out of date by increasing the modification times of their changed
            dependencies as necessary""")
    parser.add_argument('-x', '--force-expensive', action='store_true',
            help="""also rebuild targets marked as expensive""")
    parser.add_argument('target', nargs='*', help="""the target(s) to produce
            - if omitted, default target from Producefile is used""")
    return parser.parse_args(args)

### PRODUCEFILE PARSING

SECTIONHEAD_PATTERN = re.compile(r'^\[(.*)\]\s*$')
AVPAIR_PATTERN = re.compile(r'^(\S+?)\s*=\s*(.*)$', re.DOTALL)
VALUECONT_PATTERN = re.compile(r'^(\s)(.*)$', re.DOTALL)
COMMENT_PATTERN = re.compile(r'^\s*#.*$')
BLANKLINE_PATTERN = re.compile(r'^\s*$')

def parse_inifile(f):
    in_block = False
    current_name = None
    current_avpairs = None
    def current_section():
        return current_name, [(a, v.strip()) for (a, v) in current_avpairs]
    def extend_current_value(string):
        a, v = current_avpairs.pop()
        current_avpairs.append((a, v + string))
    for lineno, line in enumerate(f, start=1):
        match = SECTIONHEAD_PATTERN.match(line)
        if match:
            if in_block:
                yield current_section()
            in_block = True
            current_name = match.group(1)
            current_avpairs = []
            continue
        match = COMMENT_PATTERN.match(line)
        if match:
            continue
        if in_block:
            match = AVPAIR_PATTERN.match(line)
            if match:
                current_avpairs.append((match.group(1), match.group(2)))
                current_valuecont_pattern = VALUECONT_PATTERN
                continue
            if current_avpairs:
                match = current_valuecont_pattern.match(line)
                if match:
                    # Modify pattern for value continutation lines to cut off 
                    # only as much whitespace as the first continutation line
                    # starts with:
                    current_valuecont_pattern = re.compile(
                            r'^(' + match.group(1) + r')(.*)$', re.DOTALL)
                    extend_current_value(match.group(2))
                    continue
        match = BLANKLINE_PATTERN.match(line)
        if match:
            if current_avpairs:
                extend_current_value(os.linesep) 
            continue
        raise ProduceError('invalid line {} in Producefile'.format(lineno))
    if in_block:
        yield current_section()

def interpret_sections(sections):
    raw_globes = {}
    rules = []
    at_beginning = True
    for name, avpairs in sections:
        if at_beginning:
            at_beginning = False
            if name == '':
                raw_globes = OrderedDict(avpairs)
                continue
        if name == '':
            raise ProduceError('The global section (headed []) is only allowed'
                    ' at the beginning of the Producefile.')
        else:
            rules.append((section_name_to_regex(name), OrderedDict(avpairs)))
    return raw_globes, rules

### PATTERN MATCHING AND INTERPOLATION

def interpolate(string, varz, ignore_undefined=False,
        keep_escaped=False):
    logging.debug('interpolate called with varz: %s', {k: v for (k, v) in
            varz.items() if k != '__builtins__'})
    original_string = string
    result = ''
    while string:
        if string.startswith('%%'):
            if keep_escaped:
                result += '%%'
            else:
                result += '%'
            string = string[2:]
        elif string.startswith('%{'):
            # HACK: find the } that terminates the expansion by calling eval on
            # possible expressions of increasing length until we find one that
            # doesn't raise a syntax error. FIXME: things will still blow up if
            # the Python expression contains a comment that contains a }.
            start = 2
            error = None
            while '}' in string[start:]:
                logging.debug('looking for } in %s, starting at %s', string,
                        start)
                index = string.index('}', start)
                # Add parentheses so users don't need to add them for sequence
                # expressions:
                expression = '(' + string[2:index] + ')'
                try:
                    value = eval(expression, varz)
                    logging.debug('interpolating %s into %s', expression, value)
                    result += value_to_string(value)
                    string = string[index + 1:]
                    break
                except SyntaxError as e:
                    error = e
                except NameError as e:
                    if ignore_undefined:
                        result += string[:index + 1]
                        string = string[index + 1:]
                        break
                    else:
                        raise ProduceError(('name error in expression "{}" ' +
                                'in string "{}": {}').format(expression,
                                original_string, e))
                start = index + 1
            else:
                if error:
                    raise ProduceError(('{} evaluating expression "{}" in ' +
                            'pattern "{}": {}').format(error.__class__.__name__,
                            expression, original_string, error))
                else:
                    raise ProduceError(
                            'could not parse expression in string {}'.format(
                            original_string))
        elif string.startswith('%'):
            raise ProduceError('% must be followed by % or variable name in ' +
                    'curly braces in pattern {}'.format(original_string))
        else:
            result += string[:1]
            string = string[1:]
    return result

def value_to_string(value):
    if isinstance(value, str):
        return value
    try:
        return shlex_join(value)
    except TypeError:
        return str(value)

def section_name_to_regex(name, globes={}):
    if len(name) > 1 and name.startswith('/') and name.endswith('/'):
        try:
            return re.compile(name[1:-1])
        except Exception as e:
            raise ProduceError('{} in regex {}'.format(e, name))
    else:
        return produce_pattern_to_regex(name, globes)

def produce_pattern_to_regex(pattern, globes={}):
    ip = interpolate(pattern, globes, ignore_undefined=True,
            keep_escaped=True)
    regex = ''
    while ip:
        if ip.startswith('%%'):
            regex += '%'
            ip = ip[:2]
        # FIXME using the same variable twice in the pattern can be useful but
        # the re library will raise an exception
        elif ip.startswith('%{') and '}' in ip:
            # TODO check that the part between curly braces is a valid
            # Python identifier (or, eventually, expression)
            index = ip.index('}')
            variable = ip[2:index]
            regex += '(?P<' + variable + '>.*)'
            ip = ip[index + 1:]
        else:
            regex += re.escape(ip[:1])
            ip = ip[1:]
    regex += '$' # re.match doesn't enforce reaching the end by itself
    logging.debug('generated regex: %s', regex)
    return re.compile(regex)

### INSTANTIATED RULES

# An instantiated rule is represented by a dict that corresponds to one rule
# section from the Producefile, with the values already expanded. There are two
# special keys, target (the target as matched by the section header), and type,
# which must be one of file and task and defaults to file.

def create_irule(target, rules, globes):
    # Go through rules until a pattern matches the target:
    for pattern, avdict in rules:
        match = pattern.match(target)
        if match:
            # Dictionary representing the instantiated rule:
            result = {}
            # Dictionary for local variables:
            # TODO is the empty string a good default?
            varz = dict(globes, **match.groupdict(default = ''))
            # Special attribute: target
            result['target'] = target
            varz['target'] = target
            # Process attributes and their values:
            for attribute, value in avdict.items():
                # Remove prefix from attribute to get local variable name:
                loke = attribute.split('.')[-1]
                if loke == 'target':
                    raise ProduceError('cannot overwrite "target" attribute ' +
                            'in rule for {}'.format(target))
                # Do expansions in value:
                iv = interpolate(value, varz)
                # Attribute retains prefix:
                result[attribute] = iv
                # Local variable does not retain prefix:
                varz[loke] = iv
                # If there is a condition and it isn't met, we stop processing
                # attributes so they don't raise errors:
                if attribute == 'cond' and not ast.literal_eval(iv):
                    break
            # If there is a condition and it isn't met, go to the next rule:
            if 'cond' in result and not ast.literal_eval(result['cond']):
                logging.debug('condition %s failed, trying next rule', result['cond'])
                continue
            logging.debug('instantiated rule for target %s has varz: %s',
                    target, {k: v for (k, v) in varz.items() if k !=
                    '__builtins__'})
            result['type'] = result.get('type', 'file')
            logging.debug('target type: %s', result['type'])
            if result['type'] not in ('file', 'task'):
                raise ProduceError('unknown type {} for target {}'.format(
                    target_type, target))
            return result
        else:
            logging.debug('pattern %s did not match, trying next rule', pattern)
    if os.path.exists(target):
        # Although there is no rule to make the target, the target is a file
        # that exists, so we can use it as an ingredient.
        return {'target': target, 'type': 'file'}
    raise ProduceError('no rule to produce {}'.format(target))

def list_ddeps(irule):
    result = []
    for key, value in irule.items():
        if key.startswith('dep.'):
            result.append(value)
        elif key == 'deps':
            result.extend(shlex.split(value))
        elif key == 'depfile':
            try:
                result.extend(read_depfile(value))
            except IOError as e:
                raise ProduceError('cannot read depfile {} for target {}' \
                        .format(value, irule['target']), e)
    return result

def read_depfile(filename):
    with open(filename) as f:
        return list(map(str.strip, f))

### PRODUCTION

class Producer(Thread):

    """
    A producer is an "actor" in charge of producing a single target. It is a
    thread to be started, then joined. Afterwards, the get_result method can be
    used to get the result.
    """

    def __init__(self, production, target, depth=0):
        Thread.__init__(self)
        self.production = production
        self.target = target
        self.depth = depth
        self.exception = None

    def run(self):
        try:
            self.result = self.build_if_required()
        except Exception as e:
            # Here we could store the stacktrace in case we want to inspect it
            # the calling thread, but currently it's not needed.
            self.exception = e

    def build_if_required(self):
        """
        (Re)builds the target of this producer unless it is already up to date.
        This method is mainly for checking up-to-dateness and co-ordinating
        with other Producers that are running concurrently. The core work is
        delegated to _build.
        """
        outputs = sorted(set(self.production.target_outputs[self.target] + [
                self.target]))
        with ExitStack() as stack: # TODO ExitStack only in 3.3 and above, can we use something else?
            # Acquire lock for all outputs. Order is important to avoid
            # deadlocks. Release is automatic on leaving the context of stack.
            for output in outputs:
                stack.enter_context(self.production.target_lock[output])
            with self.production.fields_lock:
                # If the target is up-to-date and non-missing, we're done:
                if not (self.production.is_out_of_date(self.target) or \
                        self.production.is_missing(self.target)):
                    return False
                # If building the target encountered an exception before, we also
                # raise it:
                if self.target in self.production.target_exception:
                    raise self.production.target_exception[self.target]
            # Build the target:
            try:
                self._build()
            except Exception as e:
                with self.production.fields_lock:
                    self.production.target_exception[output] = e
                raise e
            # All outputs are now up-to-date and non-missing:
            with self.production.fields_lock:
                for output in outputs:
                    self.production.out_of_date_targets.discard(output)
                    self.production.missing_targets.discard(output)
            return True

    def _build(self):
        """
        If it has been decided that a target is out of date, this is the
        method that (re)builds it, unless we use the -u feature and pretend the
        target is up to date. As a prerequisite, the method first makes sure
        all direct dependencies are up to date by spawning and joining
        corresponding Producers.
        """
        if self.target in self.production.pretend_up_to_date:
            return # TODO need to raise error somewhere if target is a file and doesn't exist
        # Build required dependencies in parallel:
        threads = []
        for ddep in self.production.target_ddeps[self.target]:
            threads.append(Producer(self.production, ddep, depth=self.depth + 1))
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        for thread in threads:
            thread.get_result() # in order to propagate any exceptions
        # Run recipe. Semaphore limits number of parallel recipes:
        with self.production.build_sema:
            self._run_recipe()

    def _run_recipe(self):
        """
        This is where it really happens: the recipe for a target is run, and we
        tell the use about it.
        """
        irule = self.production.target_irule[self.target]
        if not 'recipe' in irule:
            return
        target = irule['target']
        if irule['type'] == 'task':
            logger.info(TargetLogMessage('running task', target, self.depth))
        else:
            if os.path.exists(target):
                logger.info(TargetLogMessage('rebuilding file', target,
                        self.depth))
            else:
                logger.info(TargetLogMessage('building file', target,
                        self.depth))
        recipe = irule['recipe']
        executable = irule.get('shell', 'bash')
        if recipe.startswith('\n'):
            recipe = recipe[1:]
        if not self.production.silent:
            print(recipe)
        if not self.production.dry_run:
            # Remove old backup files, if any:
            if irule['type'] == 'file':
                backup_name = target + '~'
                remove_if_exists(backup_name)
            for output in self.production.target_outputs[target]:
                backup_name = output + '~'
                remove_if_exists(backup_name)
            with NamedTemporaryFile(mode='w') as recipefile:
                recipefile.write(recipe)
                recipefile.flush()
                interrupted = False
                # Mark the files we are about to create/update as incomplete
                # so cleanup deals with them in case of an error:
                with self.production.fields_lock:
                    if irule['type'] == 'file':
                        self.production.incomplete_files.add(target)
                    for output in self.production.target_outputs[target]:
                        self.production.incomplete_files.add(output)
                # Run the recipe:
                exit = subprocess.call([executable, recipefile.name],
                        stdin=sys.stdin)
            if exit != 0:
                raise ProduceError('recipe failed: {}'.format(recipe))
            # Files were successfully created/updated and are no longer
            # incomplete:
            with self.production.fields_lock:
                if irule['type'] == 'file':
                    self.production.incomplete_files.discard(target)
                for output in self.production.target_outputs[target]:
                    self.production.incomplete_files.discard(output)
            # TODO check here if file was created for file rules, raise exception if not?  
    
    def get_result(self):
        """
        Returns True if the target was (re)built, False if the target was
        already update. Raises a ProduceError if something went wrong.
        """
        if self.exception:
            raise self.exception
        return self.result

class Production:

    """
    An object of this class represents one run of produce. The methods are the
    building blocks of the algorithm, the attributes represent options and
    state that changes during the production process.
    """

    def __init__(self, targets, rules, globes, dry_run, always_build, jobs,
            pretend_up_to_date, silent, force_expensive):
        # General properties, passed as arguments to constructor:
        self.rules = rules # maps patterns to key-uninstantiated value maps ("rules")
        self.globes = globes
        self.dry_run = dry_run
        self.always_build = always_build
        self.pretend_up_to_date = pretend_up_to_date
        self.silent = silent
        self.force_expensive = force_expensive
        # Create locks for the building phase:
        self.fields_lock = RLock()
        self.target_lock = defaultdict(RLock)
        logging.debug('using up to %s threads', jobs)
        self.build_sema = BoundedSemaphore(jobs)
        # Initialize fields to store information about individual targets:
        self.targets = []
        self.target_outputs = {}
        self.target_irule = {} # maps targets to key-instantiated value maps ("instantiated rules")
        self.target_ddeps = {} # direct dependencies
        self.target_time = {}
        self.out_of_date_targets = set()
        self.missing_targets = set()
        self.expensive_targets = set()
        self.incomplete_files = set()
        # If a recipe for a target fails, store the exception for the benefit
        # of other threads that need the same target. Instead of trying to
        # build it again, they can just raise the existing exception.
        self.target_exception = {}
        # Maps a dependent to a changed direct dependency that causes it to be
        # out of date:
        self.changed_ddeps = {}

    def add_target(self, target, beam):
        # Catch cyclic dependencies:
        if target in beam:
            raise ProduceError('cyclic dependency: {}'.format(' -> '.join(beam)
                    ))
        # Stop on encountering a target for the second time:
        if target in self.targets:
            return
        # Make instantiated rule:
        irule = create_irule(target, self.rules, self.globes)
        # Determine outputs:
        if 'outputs' in irule:
            outputs = shlex.split(irule['outputs'])
        else:
            outputs = []
        # Catch "soft cyclic dependencies":
        for output in outputs:
            if output in beam:
                raise ProduceError(
                        'cyclic dependency: {}, which has {} as output'.format(
                        ' -> '.join(beam), output))
        # Determine if expensive:
        expensive = ast.literal_eval(irule.get('expensive', 'False'))
        # The first direct dependency is the depfile, if any:
        if 'depfile' in irule:
            ddep = irule['depfile']
            # Add it recursively:
            self.add_target(ddep, beam + [target])
            # Make sure it's up to date:
            Producer(self, ddep).build_if_required() # TODO do this asynchronously like for normal targets
        # Determine other dependencies and add them recursively:
        ddeps = list_ddeps(irule)
        logging.debug('%s <- %s', target, ddeps)
        for ddep in ddeps:
            self.add_target(ddep, beam + [target])
        if ddeps:
            max_ddep_time = max([self.target_time[ddep] for ddep in ddeps])
        else:
            max_ddep_time = 0
        # Determine type, existence and time of target:
        target_type = irule['type']
        if target_type == 'task':
            missing = False
            time = 0
            logging.debug('%s is a task', target)
        else:
            missing = not os.path.exists(target)
            if missing:
                time = max_ddep_time
            else:
                time = mtime(target, 0)
                logging.debug('%s time: %s', target, time)
        logging.debug('%s missing? %s', target, missing)
        # Determine whether target is out of date, based on information about
        # dependencies, all of which are at this point guaranteed to have been
        # added:
        # 1. Tasks are always considered out of date.
        # 2. If any dependency is newer than the target, the target is out of
        #    date.
        # 3. If any dependency is out of date, so is the target.
        # 4. Consider it out of date if the always_build option is True.
        if self.always_build:
            logging.debug('%s is out of date because -B', target)
        out_of_date = target_type == 'task' \
                or any([(ddep not in self.pretend_up_to_date and \
                self.is_out_of_date(ddep)) for ddep in ddeps]) \
                or self.detect_changed_ddep(target, time, ddeps) \
                or self.always_build
        logging.debug('%s out of date? %s', target, out_of_date)
        # If we did not detect the target to be out of date but we remember it
        # is, caused by some specific newer direct dependency, we need to
        # rectify the situation by touching that dependency, thereby making it
        # out of date, and detectably so by future Produce invocations. This is
        # relevant in the second pass of adding dependencies, after the build
        # process, when we preserve ignored out-of-date statuses. Note that
        # touching a dependency does not risk introducing a new inconsistency
        # because if the dependency were out of date, we would have detected
        # this target to be out of date, too. Note also that if there is no
        # direct newer file dependency but we still remember the target as out
        # of date, we are guaranteed to detect this in a future invocation in
        # some other way - either via an intermediate dependency or because
        # there is a direct task dependency.
        if not out_of_date and target in self.changed_ddeps:
            touch(self.changed_ddeps[target], now() + 1)
        # Store information about this target:
        self.targets.append(target)
        self.target_irule[target] = irule
        self.target_outputs[target] = outputs
        self.target_ddeps[target] = ddeps
        self.target_time[target] = time
        if out_of_date:
            self.out_of_date_targets.add(target)
        if missing:
            self.missing_targets.add(target)
        if expensive:
            self.expensive_targets.add(target)

    def detect_changed_ddep(self, target, time, ddeps):
        for ddep in ddeps:
            if self.target_time[ddep] > time:
                self.changed_ddeps[target] = ddep
                return True
        return False

    def is_out_of_date(self, target):
        return target in self.out_of_date_targets

    def is_missing(self, target):
        return target in self.missing_targets

    def produce(self, targets):
        try:
            # Phase 1: build dependency graph and gather information
            for target in targets:
                self.add_target(target, [])
            # Phase 1.5: check for out-of-date expensive targets
            if not self.force_expensive:
                danger = (self.out_of_date_targets & self.expensive_targets) - \
                        self.pretend_up_to_date
                if danger:
                    raise ProduceError(('The following targets are out of date '
                            'and marked as expensive:\n{}\nAvoid rebuilding '
                            'with -u and/or force rebuilding of expensive '
                            'targets with -x.').format('\n'.join(danger)))
            # Phase 2: build all we need to build
            threads = []
            for target in targets:
                threads.append(Producer(self, target))
            for thread in threads:
                thread.start()
            for thread in threads:
                thread.join()
            if all([not thread.get_result() for thread in threads]):
                logging.info('all targets are up to date')
        finally:
            # Phase 3: rename potentially incomplete files
            for output in self.incomplete_files:
                backup_name = output + '~'
                logging.debug('renaming %s to %s', output, backup_name)
                rename_if_exists(output, backup_name)
            # Phase 4: repeat phase 1 for the pretend targets, touching files
            # in order to preserve out-of-dateness
            self.targets = [] # circumvent add_target's deduplication
            for target in self.pretend_up_to_date:
                self.add_target(target, [])

### MAIN

def produce(args=[]):
    args = process_commandline(args)
    if args.debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logging.basicConfig(format='%(levelname)s: %(message)s',
            level=level)
    try:
        try:
            with open(args.file) as f:
               sections = list(parse_inifile(f))
               logging.debug('parsed sections: %s', sections)
               raw_globes, rules = interpret_sections(sections)
        except IOError as e:
            raise ProduceError('cannot read file {}'.format(args.file), e)
        globes = {}
        for att, val in raw_globes.items():
            globes[att] = interpolate(val, globes)
        # Determine targets:
        targets = args.target
        if not targets:
            if 'default' in globes:
                targets = shlex.split(globes['default'])
            else:
                raise ProduceError('Don\'t know what to produce. Specify a ' +
                        'target on the command line or a default target in ' +
                        'produce.ini.')
        # Execute prelude, with globes providing the name context:
        exec(globes.get('prelude', ''), globes)
        # Produce:
        Production(targets, rules, globes, args.dry_run, args.always_build,
                args.jobs, args.pretend_up_to_date, args.silent,
                args.force_expensive).produce(targets)
    except KeyboardInterrupt as e:
        raise ProduceError('interrupted')

if __name__ == '__main__':
    try:
        produce(None)
    except ProduceError as e:
        logging.error(e) # FIXME prints only first line of error message???
        sys.exit(1)
