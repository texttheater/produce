#!/usr/bin/env python3

"""
Produce is a replacement for Make geared towards processing data rather than
compiling code. Key feature: supports multiple expansions in pattern rules.
https://github.com/texttheater/produce/
"""

import argparse
import ast
import asyncio
import atexit
import concurrent
import errno
import logging
import os
import re
import shlex
import shutil
import signal
import subprocess
import sys
import time

from asyncio import BoundedSemaphore, Lock
from collections import defaultdict, OrderedDict
from contextlib import ExitStack
from tempfile import NamedTemporaryFile


### PLANNED FEATURES


# TODO change log output to be both friendlier to the human eye and more
# machine-readable. Then write unit tests that check for not doing the same
# thing twice, e.g. when two targets are outputs of the same IRule or when a
# recipe fails for a target that multiple other targets depend on. Then,
# finally, merge feat/outputs into master.
# TODO includes
# TODO variation on the file rule type that unconditionally runs the recipe
# but with a temp file with target, then percolates dirtiness only if the new
# file differs from the existing version - e.g. for files that depend on data
# from a database.
# TODO flat producing: run the recipe of the target, touch all existing
# dependencies
# TODO optionally delete intermediate files
# TODO rename private members


### CLEANUP

# Make sure the event loop is closed before the program exits so we don't get
# a resource warning or hit this bug: https://bugs.python.org/issue23548
# Also need to handle the case where the event loop is already gone for some
# reason. asyncio does make one jump through hoops. >:-(


def close_event_loop():
    try:
        loop = asyncio.get_event_loop()
    except (RuntimeError, AssertionError):
        return
    loop.close()


atexit.register(close_event_loop)


### UTILITIES


def get_results(tasks):
    """
    Returns a list of the results of the given asyncio.Task objects. It also
    makes sure *all* their exceptions are retrieved, but only the first is
    re-raised.
    """
    exception = None
    results = []
    for task in tasks:
        try:
            results.append(task.result())
        except BaseException as e:
            if exception is None:
                exception = e
    if exception is not None:
        raise exception
    return results


def have_smart_terminal():
    # courtesy of apenwarr's redo
    return (os.environ.get('TERM') or 'dumb') != 'dumb'


def mtime(path, default=0):
    """
    Returns the mtime of the file at the specified path. Returns default if
    file does not exist. An OSError is raised if the file cannot be stat'd.
    """
    try:
        return os.stat(path).st_mtime
    except OSError as e:
        if e.errno == errno.ENOENT:
            return default
        raise e


def now():
    return time.time()


def remove_if_exists(path):
    try:
        os.remove(path)
    except OSError as e:
        if e.errno == errno.EISDIR:
            shutil.rmtree(path)
        elif e.errno == errno.ENOENT:
            pass  # Mission. F***ing. Accomplished.
        else:
            raise e


def rename_if_exists(src, dst):
    try:
        os.rename(src, dst)
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise e


def touch(path, timestamp):
    if subprocess.call(['touch', '-d', '@{}'.format(timestamp), path]) != 0:
        raise ProduceError('failed to touch {}'.format(path))


def shlex_join(value):
    return ' '.join((shlex.quote(str(x)) for x in value))


### GENERAL LOGGING

# General logging uses the root logger and configures it for each production
# (using the set_up_logging function). We use log levels INFO and DEBUG. For
# DEBUG messages, we also roll our own, finer-grained filter depending on how
# detailed debug output the user requested.


def set_up_logging(dbglvl):
    global debug_level
    debug_level = dbglvl
    if debug_level > 1:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logging.basicConfig(format='%(levelname)s: %(message)s', level=level)


def debug(level, message_format, *parameters):
    if debug_level >= level:
        logging.debug(message_format, *parameters)


### LOGGING FOR STATUS MESSAGES

# Status messages also use the logging module, but a different logger, called
# produce. Some unit tests hook into this logger using assertLogs to test that
# the right status messages are printed at the right time. The methods
# status_info and status_error are used to generate status messages.


class StatusMessage():

    def __init__(self, action, target, depth):
        self.action = action
        self.target = target
        self.depth = depth

    def __str__(self):
        return '{} {}'.format(self.action, self.target)


class StatusFormatter(logging.Formatter):

    def __init__(self, colors):
        logging.Formatter.__init__(self)
        if colors:
            self.red    = '\x1b[31m'
            self.green  = '\x1b[32m'
            self.yellow = '\x1b[33m'
            self.bold   = '\x1b[1m'
            self.plain  = '\x1b[m'
        else:
            self.red = ''
            self.green = ''
            self.yellow = ''
            self.bold = ''
            self.plain = ''

    def format(self, record):
        if isinstance(record.msg, StatusMessage):
            if record.levelno == logging.ERROR:
                color = self.red
            else:
                color = self.green
            return '{}{}{}{}{}{}{}'.format(
                color, record.msg.action,
                ' ' * (16 - len(record.msg.action)), self.bold,
                ' ' * record.msg.depth, record.msg.target, self.plain)
        else:
            return logging.Formatter.format(self, record)


# Set up logging for status messages:
status_logger = logging.getLogger('produce')
status_logger.propagate = False
status_logger.handlers = []  # remove default handler
status_logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
handler.setLevel(logging.INFO)
formatter = StatusFormatter(sys.stderr.isatty() and have_smart_terminal())
handler.setFormatter(formatter)
status_logger.addHandler(handler)


def status_info(message, target, depth):
    global status_logger
    status_logger.info(StatusMessage(message, target, depth))


def status_error(message, target, depth):
    global status_logger
    status_logger.error(StatusMessage(message, target, depth))


### ERRORS


class ProduceError(Exception):

    def __init__(self, message, cause=None):
        Exception.__init__(self, message)
        self.cause = cause


### COMMANDLINE PROCESSING


def process_commandline(args=None):
    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group()
    group.add_argument(
        '-B', '--always-build', action='store_true',
        help="""Unconditionally build all specified targets and their
        dependencies""")
    group.add_argument(
        '-b', '--always-build-specified', action='store_true',
        help="""Unconditionally build all specified targets, but treat their
        dependencies normally (only build if out of date)""")
    parser.add_argument(
        '-d', '--debug', action='count', default=0,
        help="""Print debugging information. Give this option multiple times
        for more information.""")
    parser.add_argument(
        '-f', '--file', default='produce.ini',
        help="""Use FILE as a Producefile""")
    parser.add_argument(
        '-j', '--jobs', type=int, default=1,
        help="""Specifies the number of jobs (recipes) to run
        simultaneously""")
    parser.add_argument(
        '-n', '--dry-run', action='store_true',
        help="""Print status messages, but do not run recipes""")
    parser.add_argument(
        '-u', '--pretend-up-to-date', metavar='FILE', action='append',
        default=[],
        help="""Do not rebuild FILE or its dependencies (unless they are also
        depended on by other targets) even if out of date, but make sure that
        future invocations of Produce will still treat them as out of date by
        increasing the modification times of their changed dependencies as
        necessary.""")
    parser.add_argument(
        'target', nargs='*',
        help="""The target(s) to produce - if omitted, default target from
        Producefile is used""")
    return parser.parse_args(args)


### PRODUCEFILE PARSING


SECTIONHEAD_PATTERN = re.compile(r'^\[(.*)\]\s*$')
AVPAIR_PATTERN = re.compile(r'^(\S+?)\s*=\s*(.*)$', re.DOTALL)
VALUECONT_PATTERN = re.compile(r'^(\s)(.*)$', re.DOTALL)
COMMENT_PATTERN = re.compile(r'^\s*#.*$')
BLANKLINE_PATTERN = re.compile(r'^\s*$')


def parse_inifile(f):
    in_block = False
    current_name = None
    current_avpairs = None

    def current_section():
        return current_name, [(a, v.strip()) for (a, v) in current_avpairs]

    def extend_current_value(string):
        a, v = current_avpairs.pop()
        current_avpairs.append((a, v + string))

    for lineno, line in enumerate(f, start=1):
        match = SECTIONHEAD_PATTERN.match(line)
        if match:
            if in_block:
                yield current_section()
            in_block = True
            current_name = match.group(1)
            current_avpairs = []
            continue
        match = COMMENT_PATTERN.match(line)
        if match:
            continue
        if in_block:
            match = AVPAIR_PATTERN.match(line)
            if match:
                current_avpairs.append((match.group(1), match.group(2)))
                current_valuecont_pattern = VALUECONT_PATTERN
                continue
            if current_avpairs:
                match = current_valuecont_pattern.match(line)
                if match:
                    # Modify pattern for value continutation lines to cut off
                    # only as much whitespace as the first continutation line
                    # starts with:
                    current_valuecont_pattern = re.compile(
                        r'^(' + match.group(1) + r')(.*)$', re.DOTALL)
                    extend_current_value(match.group(2))
                    continue
        match = BLANKLINE_PATTERN.match(line)
        if match:
            if current_avpairs:
                extend_current_value(os.linesep)
            continue
        raise ProduceError('invalid line {} in Producefile'.format(lineno))
    if in_block:
        yield current_section()


def interpret_sections(sections):
    raw_globes = {}
    rules = []
    at_beginning = True
    for name, avpairs in sections:
        if at_beginning:
            at_beginning = False
            if name == '':
                raw_globes = OrderedDict(avpairs)
                continue
        if name == '':
            raise ProduceError('The global section (headed []) is only allowed'
                               ' at the beginning of the Producefile.')
        else:
            rules.append((section_name_to_regex(name), OrderedDict(avpairs)))
    return raw_globes, rules


### PATTERN MATCHING AND INTERPOLATION


def interpolate(string, varz, ignore_undefined=False, keep_escaped=False):
    debug(3, 'interpolate called with varz: %s', {k: v for (k, v) in
          varz.items() if k != '__builtins__'})
    original_string = string
    result = ''
    while string:
        if string.startswith('%%'):
            if keep_escaped:
                result += '%%'
            else:
                result += '%'
            string = string[2:]
        elif string.startswith('%{'):
            # HACK: find the } that terminates the expansion by calling eval on
            # possible expressions of increasing length until we find one that
            # doesn't raise a syntax error. FIXME: things will still blow up if
            # the Python expression contains a comment that contains a }.
            start = 2
            error = None
            while '}' in string[start:]:
                debug(3, 'looking for } in %s, starting at %s', string, start)
                index = string.index('}', start)
                # Add parentheses so users don't need to add them for sequence
                # expressions:
                expression = '(' + string[2:index] + ')'
                try:
                    value = eval(expression, varz)
                    debug(3, 'interpolating %s into %s', expression, value)
                    result += value_to_string(value)
                    string = string[index + 1:]
                    break
                except SyntaxError as e:
                    error = e
                except NameError as e:
                    if ignore_undefined:
                        result += string[:index + 1]
                        string = string[index + 1:]
                        break
                    else:
                        raise ProduceError(
                            'name error in expression "{}" in string "{}": {}'
                            .format(expression, original_string, e))
                start = index + 1
            else:
                if error:
                    raise ProduceError(
                        '{} evaluating expression "{}" in pattern "{}": {}'
                        .format(error.__class__.__name__, expression,
                                original_string, error))
                else:
                    raise ProduceError(
                        'could not parse expression in string {}'
                        .format(original_string))
        elif string.startswith('%'):
            raise ProduceError(
                '% must be followed by % or variable name in curly braces in '
                'pattern {}'.format(original_string))
        else:
            result += string[:1]
            string = string[1:]
    return result


def value_to_string(value):
    if isinstance(value, str):
        return value
    try:
        return shlex_join(value)
    except TypeError:
        return str(value)


def section_name_to_regex(name, globes={}):
    if len(name) > 1 and name.startswith('/') and name.endswith('/'):
        try:
            return re.compile(name[1:-1])
        except Exception as e:
            raise ProduceError('{} in regex {}'.format(e, name))
    else:
        return produce_pattern_to_regex(name, globes)


def produce_pattern_to_regex(pattern, globes={}):
    ip = interpolate(pattern, globes, ignore_undefined=True, keep_escaped=True)
    regex = ''
    while ip:
        if ip.startswith('%%'):
            regex += '%'
            ip = ip[:2]
        # FIXME using the same variable twice in the pattern can be useful but
        # the re library will raise an exception
        elif ip.startswith('%{') and '}' in ip:
            # TODO check that the part between curly braces is a valid
            # Python identifier (or, eventually, expression)
            index = ip.index('}')
            variable = ip[2:index]
            regex += '(?P<' + variable + '>.*)'
            ip = ip[index + 1:]
        else:
            regex += re.escape(ip[:1])
            ip = ip[1:]
    regex += '$'  # re.match doesn't enforce reaching the end by itself
    debug(3, 'generated regex: %s', regex)
    return re.compile(regex)


### INSTANTIATED RULES

# An instantiated rule is represented by a dict that corresponds to one rule
# section from the Producefile, with the values already expanded. There are two
# special keys, target (the target as matched by the section header), and type,
# which must be one of file and task and defaults to file.


def create_irule(target, rules, globes):
    # Go through rules until a pattern matches the target:
    for pattern, avdict in rules:
        match = pattern.match(target)
        if match:
            # Dictionary representing the instantiated rule:
            result = {}
            # Dictionary for local variables:
            # TODO is the empty string a good default?
            varz = dict(globes, **match.groupdict(default=''))
            # Special attribute: target
            result['target'] = target
            varz['target'] = target
            # Process attributes and their values:
            for attribute, value in avdict.items():
                # Remove prefix from attribute to get local variable name:
                loke = attribute.split('.')[-1]
                if loke == 'target':
                    raise ProduceError('cannot overwrite "target" attribute '
                                       'in rule for {}'.format(target))
                # Do expansions in value:
                iv = interpolate(value, varz)
                # Attribute retains prefix:
                result[attribute] = iv
                # Local variable does not retain prefix:
                varz[loke] = iv
                # If there is a condition and it isn't met, we stop processing
                # attributes so they don't raise errors:
                if attribute == 'cond' and not ast.literal_eval(iv):
                    break
            # If there is a condition and it isn't met, go to the next rule:
            if 'cond' in result and not ast.literal_eval(result['cond']):
                debug(3, 'condition %s failed, trying next rule',
                      result['cond'])
                continue
            debug(3, 'instantiated rule for target %s has varz: %s', target,
                  {k: v for (k, v) in varz.items() if k != '__builtins__'})
            result['type'] = result.get('type', 'file')
            debug(3, 'target type: %s', result['type'])
            if result['type'] not in ('file', 'task'):
                raise ProduceError('unknown type {} for target {}'.format(
                    target_type, target))
            return result
        else:
            debug(3, 'pattern %s did not match, trying next rule', pattern)
    if os.path.exists(target):
        # Although there is no rule to make the target, the target is a file
        # that exists, so we can use it as an ingredient.
        return {'target': target, 'type': 'file'}
    raise ProduceError('no rule to produce {}'.format(target))


def list_ddeps(irule):
    result = []
    for key, value in irule.items():
        if key.startswith('dep.'):
            result.append(value)
        elif key == 'deps':
            result.extend(shlex.split(value))
        elif key == 'depfile':
            try:
                result.extend(read_depfile(value))
            except IOError as e:
                raise ProduceError(
                    'cannot read depfile {} for target {}'
                    .format(value, irule['target']), e)
    return result


def read_depfile(filename):
    with open(filename) as f:
        return list(map(str.strip, f))


### PRODUCTION


class Producer:

    """
    A producer is an "actor" in charge of producing a single target. This is
    done by running its coroutine build_if_required to completion.
    """

    def __init__(self, production, target, depth=0):
        self.production = production
        self.target = target
        self.depth = depth
        self.exception = None

    @asyncio.coroutine
    def build_if_required(self):
        """
        (Re)builds the target of this producer unless it is already up to date.
        This method is mainly for up-to-dateness bookkeeping and co-ordinating
        with other Producers that are running concurrently. The core work is
        delegated to _build.
        """
        # We lock all side outputs.
        outputs = set(self.production.target_outputs[self.target])
        lockables = set(outputs)
        # We also lock the target iff we have a recipe for it:
        if self.production.target_irule[self.target].get('recipe'):
            lockables.add(self.target)
        else:
            lockables.discard(self.target)  # in case it's in outputs
        # Sort lockables to prevent deadlocks:
        lockables = sorted(lockables)
        with ExitStack() as stack:  # TODO ExitStack only in 3.3 and above, can we use something else?
            # Acquire locks. Release is automatic on leaving the context of
            # stack.
            for lockable in lockables:
                debug(3, 'locking %s', lockable)
                lock = self.production.target_lock[lockable]
                stack.enter_context((yield from lock))
                debug(3, 'locked %s', lockable)
            # Check if the target still needs to be built:
            # If the target is up-to-date and non-missing, we're done:
            if not (self.production.is_out_of_date(self.target) or
                    self.production.is_missing(self.target)):
                return False
            # If building this target encountered an exception before, we
            # also raise it, in case this coroutine completes earlier than the
            # one that originally raised it. TODO Is that actually possible?
            if self.target in self.production.target_exception:
                raise self.production.target_exception[self.target]
            # Build the target:
            try:
                yield from self._build()
            except Exception as e:
                self.production.target_exception[self.target] = e
                raise e
            # Mark all outputs as now up-to-date and non-missing:
            for output in outputs | set((self.target,)):
                self.production.out_of_date_targets.discard(output)
                self.production.missing_targets.discard(output)
                self.production.built_targets.add(self.target)
        return True

    @asyncio.coroutine
    def _build(self):
        """
        If it has been decided that a target is out of date, this is the
        method that (re)builds it, unless we use the -u feature and pretend the
        target is up to date. As a prerequisite, the method first makes sure
        all direct dependencies are up to date by recursively calling _produce.
        """
        if self.target in self.production.pretend_up_to_date:
            return  # TODO need to raise error somewhere if target is a file and doesn't exist
        # Make sure dependencies are up to date:
        yield from self.production._produce(
            self.production.target_ddeps[self.target],
            depth=self.depth + 1)
        # Run recipe. Semaphore limits number of parallel recipes:
        with (yield from self.production.build_sema):
            yield from self._run_recipe()

    @asyncio.coroutine
    def _run_recipe(self):
        """
        This is where it really happens: the recipe for a target is run, and we
        tell the user about it.
        """
        if self.production.killed:
            return
        irule = self.production.target_irule[self.target]
        if not 'recipe' in irule:
            return
        target = irule['target']
        if irule['type'] == 'task':
            status_info('running task', target, self.depth)
        else:
            if os.path.exists(target):
                status_info('rebuilding file', target, self.depth)
            else:
                status_info('building file', target, self.depth)
        recipe = irule['recipe']
        executable = irule.get('shell', 'bash')
        if recipe.startswith('\n'):
            recipe = recipe[1:]
        if debug_level >= 1:
            print(recipe)
        if not self.production.dry_run:
            # Remove old backup files, if any:
            if irule['type'] == 'file':
                backup_name = target + '~'
                remove_if_exists(backup_name)
            for output in self.production.target_outputs[target]:
                backup_name = output + '~'
                remove_if_exists(backup_name)
            success = False
            try:
                with NamedTemporaryFile(mode='w') as recipefile:
                    recipefile.write(recipe)
                    recipefile.flush()
                    process = yield from asyncio.create_subprocess_exec(
                        executable, recipefile.name)
                    yield from asyncio.wait(
                        (process.wait(),
                         self.production.supervise(process)))
                    debug(3, 'Exit code: %s', process.returncode)
                if process.returncode != 0:
                    # Kill the production to prevent new recipes from running
                    # and the cause the supervisor routines to kill running
                    # ones:
                    self.production.killed = True
                    raise ProduceError('recipe failed: {}'.format(recipe))
                success = True
                # TODO check here if file was created for file rules, raise
                # exception if not?
            finally:
                if not success:
                    if irule['type'] == 'file':
                        backup_name = target + '~'
                        debug(2, 'renaming %s to %s', target, backup_name)
                        rename_if_exists(target, backup_name)
                    for output in self.production.target_outputs[target]:
                        backup_name = output + '~'
                        debug(2, 'renaming %s to %s', output, backup_name)
                        rename_if_exists(output, backup_name)
                    status_error('incomplete', target, self.depth)
            status_info('complete', target, self.depth)


class Production:

    """
    An object of this class represents one run of produce. The methods are the
    building blocks of the algorithm, the attributes represent options and
    state that changes during the production process.
    """

    def __init__(self, targets, rules, globes, dry_run, always_build, jobs,
                 pretend_up_to_date):
        # General properties, passed as arguments to constructor:
        self.rules = rules  # maps patterns to key-uninstantiated value maps ("rules")
        self.globes = globes
        self.dry_run = dry_run
        self.always_build = always_build
        self.pretend_up_to_date = pretend_up_to_date
        # Create locks for the building phase:
        self.target_lock = defaultdict(Lock)
        debug(3, 'using up to %s threads', jobs)
        self.build_sema = BoundedSemaphore(jobs)
        # Initialize fields to store information about individual targets:
        self.targets = []
        self.target_outputs = {}
        self.target_irule = {}  # maps targets to key-instantiated value maps ("instantiated rules")
        self.target_ddeps = {}  # direct dependencies
        self.target_time = {}
        self.out_of_date_targets = set()
        self.missing_targets = set()
        self.built_targets = set()
        # If a recipe for a target fails, store the exception for the benefit
        # of other threads that need the same target. Instead of trying to
        # build it again, they can just raise the existing exception.
        self.target_exception = {}
        # Maps a dependent to a changed direct dependency that causes it to be
        # out of date:
        self.changed_ddeps = {}
        # Abort flag:
        self.killed = False

    def add_target(self, target, beam, always_build_specified):
        # Catch cyclic dependencies:
        if target in beam:
            raise ProduceError('cyclic dependency: {}'.format(' -> '.join(
                beam + [target])))
        # Stop on encountering a target for the second time:
        if target in self.targets:
            return
        # Make instantiated rule:
        irule = create_irule(target, self.rules, self.globes)
        # Store it directly (we need it for spotting harmless soft cycles):
        self.target_irule[target] = irule
        # Determine outputs:
        if 'outputs' in irule:
            outputs = shlex.split(irule['outputs'])
        else:
            outputs = []
        # Catch "soft cyclic dependencies". We don't want to allow a target to
        # depend on a rule that will produce it as a side output: the target
        # would be built twice by the same production, which is crazy and
        # probably indicates a bug. There's an exception though: if the target
        # has an empty recipe, it may depend on a target that produces the
        # first target as a side output. This is useful because it allows
        # dependencies on side outputs: they can then declare which target to
        # produce to get the side output.
        for output in outputs:
            if output in beam and self.target_irule[output].get('recipe'):
                raise ProduceError(
                    'cyclic dependency: {}, which has {} as output'.format(
                        ' -> '.join(beam + [target]), output))
        # The first direct dependency is the depfile, if any:
        if 'depfile' in irule:
            ddep = irule['depfile']
            # Add it recursively:
            self.add_target(ddep, beam + [target], False)
            # Make sure it's up to date:
            producer = Producer(self, ddep)
            future = producer.build_if_required()
            asyncio.get_event_loop().run_until_complete(future)
            # TODO do this asynchronously like for normal targets
        # Determine other dependencies and add them recursively:
        ddeps = list_ddeps(irule)
        debug(2, '%s <- %s', target, ddeps)
        for ddep in ddeps:
            self.add_target(ddep, beam + [target], False)
        if ddeps:
            max_ddep_time = max([self.target_time[ddep] for ddep in ddeps])
        else:
            max_ddep_time = 0
        # Determine type, existence and time of target:
        target_type = irule['type']
        if target_type == 'task':
            missing = False
            time = 0
            debug(3, '%s is a task', target)
        else:
            missing = not os.path.exists(target)
            if missing:
                time = max_ddep_time
            else:
                time = mtime(target, 0)
                debug(3, '%s time: %s', target, time)
        debug(3, '%s missing? %s', target, missing)
        # Determine whether target is out of date, based on information about
        # dependencies, all of which are at this point guaranteed to have been
        # added:
        out_of_date = self._check_if_out_of_date(target, target_type, ddeps,
                                                 time, always_build_specified)
        # Store information about this target:
        self.targets.append(target)
        self.target_outputs[target] = outputs
        self.target_ddeps[target] = ddeps
        self.target_time[target] = time
        if out_of_date:
            self.out_of_date_targets.add(target)
        if missing:
            self.missing_targets.add(target)

    def _check_if_out_of_date(self, target, target_type, ddeps, time,
                              always_build_specified):
        if target_type == 'task':
            debug(2, '%s is out of date because it is a task', target)
            return True
        for ddep in ddeps:
            if self.target_time[ddep] > time:
                debug(2, '%s is out of date because its direct dependency %s '
                      'changed', target, ddep)
                # Remember which changed ddep caused this target to be out of
                # date - relevant for pretend mode:
                self.changed_ddeps[target] = ddep
                return True
        for ddep in ddeps:
            if ddep not in self.pretend_up_to_date and \
                    self.is_out_of_date(ddep):
                debug(2, '%s is out of date because its direct dependency %s '
                      'is out of date', target, ddep)
                return True
        if self.always_build:
            debug(2, '%s is out of date because we are in "always build" mode',
                  target)
            return True
        if always_build_specified:
            debug(2, '%s is out of date because we are in "always build '
                  'specified" mode', target)
            return True
        debug(2, '%s is up to date', target)
        return False

    def is_out_of_date(self, target):
        return target in self.out_of_date_targets

    def is_missing(self, target):
        return target in self.missing_targets

    @asyncio.coroutine
    def _produce(self, targets, depth=0):
        """
        Internally used method for producing a list of targets. Returns True if
        something needed to be done, False if all targets were already up to
        date.
        """
        if not targets:
            return False  # asyncio.wait doesn't like empty sequences
        # Build requested targets in parallel:
        producers = [Producer(self, target, depth=depth) for target in targets]
        tasks = [asyncio.Task(producer.build_if_required())
                 for producer in producers]
        yield from asyncio.wait(tasks)
        return any(get_results(tasks))

    @asyncio.coroutine
    def supervise(self, process):
        """
        This coroutine periodically checks whether the Production is killed and
        if so, kills the process given as argument. It returns when the process
        is done.
        """
        while True:
            yield from asyncio.sleep(0.05)
            if process.returncode is not None:
                return
            if self.killed:
                process.kill()
                # FIXME this doesn't always kill all child processes

    def _adjust_times_for(self, target):
        for ddep in self.target_ddeps[target]:
            if ddep in self.pretend_up_to_date:
                self._adjust_times(ddep)
            else:
                self._adjust_times_for(ddep)

    def _adjust_times(self, target):
        """
        Follows the dependencies of the target down to the lowest files whose
        change caused the out-of-dateness and touch it to make sure it still
        causes the out-of-dateness even if intermediate targets are deleted.
        """
        for ddep in self.target_ddeps[target]:
            if self.is_out_of_date(ddep):
                self._adjust_times(ddep)
            if target in self.changed_ddeps \
                    and ddep == self.changed_ddeps[target]:
                debug(2, 'Adjusting time of %s', ddep)
                touch(ddep, now() + 1)

    def produce(self, targets, always_build_specified=False):
        """
        After adding all targets, call this method to produce them. High-level
        method responsible for error handling and clean aborts.
        """
        #loop = asyncio.get_event_loop()
        loop = asyncio.get_event_loop()

        def handle_signal(production):
            production.killed = True

        loop.add_signal_handler(signal.SIGTERM, handle_signal, self)
        loop.add_signal_handler(signal.SIGHUP, handle_signal, self)
        loop.add_signal_handler(signal.SIGINT, handle_signal, self)
        try:
            # Phase 1: build dependency graph and gather information
            # TODO We could probably already start building each target as soon
            # as it's added. Advantages: more elegant, dependency files could
            # be built concurrently.
            for target in targets:
                self.add_target(target, [], always_build_specified)
            debug(3, 'Changed direct dependencies: %s', self.changed_ddeps)
            # Phase 2: build all we need to build
            # TODO Here we'd then not run the producers, just wait for them to
            # finish. How? Via centrally kept queues?
            task = asyncio.Task(self._produce(targets))
            loop.run_until_complete(asyncio.wait((task,)))
            if not task.result():
                logging.info('all targets are up to date')
        finally:
            # Phase 3: adjust times of dependencies of targets that were built,
            # correcting for time inconsistencies caused by pretend targets:
            for target in self.built_targets:
                debug(2, 'Adjusting times for built target %s', target)
                self._adjust_times_for(target)


### MAIN


def produce(args=[]):
    args = process_commandline(args)
    set_up_logging(args.debug)
    try:
        with open(args.file) as f:
            sections = list(parse_inifile(f))
            debug(3, 'parsed sections: %s', sections)
            raw_globes, rules = interpret_sections(sections)
    except IOError as e:
        raise ProduceError('cannot read file {}'.format(args.file), e)
    globes = {}
    for att, val in raw_globes.items():
        globes[att] = interpolate(val, globes)
    # Determine targets:
    targets = args.target
    if not targets:
        if 'default' in globes:
            targets = shlex.split(globes['default'])
        else:
            raise ProduceError(
                "Don't know what to produce. Specify a target on the command "
                "line or a default target in produce.ini")
    # Execute prelude, with globes providing the name context:
    exec(globes.get('prelude', ''), globes)
    # Produce:
    production = Production(targets, rules, globes, args.dry_run,
                            args.always_build, args.jobs,
                            args.pretend_up_to_date)
    production.produce(targets, args.always_build_specified)


if __name__ == '__main__':
    try:
        produce(None)
    except ProduceError as e:
        logging.error(e)  # FIXME prints only first line of error message???
        sys.exit(1)
    except KeyboardInterrupt as e:
        logging.error('Interrupted')
        sys.exit(1)
