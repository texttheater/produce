#!/usr/bin/env python3

import argparse
import ast
import errno
import logging
import os
import queue
import re
import shlex
import shutil
import signal
import subprocess
import sys
import time

from collections import defaultdict, OrderedDict
from contextlib import ExitStack
from tempfile import NamedTemporaryFile
from threading import BoundedSemaphore, RLock, Thread

### PLANNED FEATURES

# TODO change log output to be both friendlier to the human eye and more
# machine-readable. Then write unit tests that check for not doing the same
# thing twice, e.g. when two targets are outputs of the same IRule or when a
# recipe fails for a target that multiple other targets depend on. Then,
# finally, merge feat/outputs into master.
# TODO includes
# TODO variation on the file rule type that unconditionally runs the recipe
# but with a temp file with target, then percolates dirtiness only if the new
# file differs from the existing version - e.g. for files that depend on data
# from a database.
# TODO flat producing: run the recipe of the target, touch all existing
# dependencies
# TODO optionally delete intermediate files
# TODO rename private members

### UTILITIES

def have_smart_terminal():
    # courtesy of apenwarr's redo
    return (os.environ.get('TERM') or 'dumb') != 'dumb'

def mtime(path, default=0):
    """
    Returns the mtime of the file at the specified path. Returns default if
    file does not exist. An OSError is raised if the file cannot be stat'd.
    """
    try:
        return os.stat(path).st_mtime
    except OSError as e:
        if e.errno == errno.ENOENT:
            return default
        raise e

def now():
    return time.time()

def remove_if_exists(path):
    try:
        os.remove(path)
    except OSError as e:
        if e.errno == errno.EISDIR:
            shutil.rmtree(path)
        elif e.errno == errno.ENOENT:
            pass # Mission. F***ing. Accomplished.
        else:
            raise e

def rename_if_exists(src, dst):
    try:
        os.rename(src, dst)
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise e

def touch(path, timestamp):
    if subprocess.call(['touch', '-d', '@{}'.format(timestamp), path]) != 0:
        raise ProduceError('failed to touch {}'.format(path))

def shlex_join(value):
    return ' '.join((shlex.quote(str(x)) for x in value))

### LOGGING
# Produce aims to communicate with the user in an inuitive and flexible way.
# Reconciling these aims with Python's logging facilities is an ongoing battle.
# The current code is terribly ugly, but at least the ugliness is now mostly
# contained within this section.

status_logger = logging.getLogger('produce')
status_logger.propagate = False
logging_is_set_up = False

def set_up_logging(dbglvl):
    # Ordinary logging:
    global debug_level
    debug_level = dbglvl
    if debug_level > 1:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logging.basicConfig(format='%(levelname)s: %(message)s', level=level)
    # Status messages:
    global logging_is_set_up
    if logging_is_set_up:
        return
    logging_is_set_up = True
    global status_logger
    status_logger.handlers = []
    status_logger.setLevel(logging.INFO)
    handler = logging.StreamHandler()
    handler.setLevel(logging.INFO)
    formatter = StatusFormatter(sys.stderr.isatty() and have_smart_terminal())
    handler.setFormatter(formatter)
    status_logger.addHandler(handler)


def debug(level, message_format, *parameters):
    if debug_level >= level:
        logging.debug(message_format, *parameters)

class StatusMessage():

    def __init__(self, action, target, depth):
        self.action = action
        self.target = target
        self.depth = depth

    def __str__(self):
        return '{} {}'.format(self.action, self.target)

class StatusFormatter(logging.Formatter):

    def __init__(self, colors):
        logging.Formatter.__init__(self)
        if colors:
            self.red    = '\x1b[31m'
            self.green  = '\x1b[32m'
            self.yellow = '\x1b[33m'
            self.bold   = '\x1b[1m'
            self.plain  = '\x1b[m'
        else:
            self.red = ''
            self.green = ''
            self.yelloe = ''
            self.bold = ''
            self.plain = ''

    def format(self, record):
        if isinstance(record.msg, StatusMessage):
            if record.levelno == logging.ERROR:
                color = self.red
            else:
                color = self.green
            return '{}{}{}{}{}{}{}'.format(color, record.msg.action,
                    ' ' * (16 - len(record.msg.action)), self.bold,
                    ' ' * record.msg.depth, record.msg.target, self.plain)
        else:
            return logging.Formatter.format(self, record)

def status_info(message, target, depth):
    global status_logger
    status_logger.info(StatusMessage(message, target, depth))

def status_error(message, target, depth):
    global status_logger
    status_logger.error(StatusMessage(message, target, depth))

### ERRORS

class ProduceError(Exception):

    def __init__(self, message, cause=None):
        Exception.__init__(self, message)
        self.cause = cause

### COMMANDLINE PROCESSING

def process_commandline(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument('-B', '--always-build', action='store_true',
            help="""Unconditionally build all targets""")
    parser.add_argument('-d', '--debug', action='count', default=0,
            help="""Print debugging information. Give this option multiple
            times for more information.""")
    parser.add_argument('-f', '--file', default='produce.ini',
            help="""Use FILE as a Producefile""")
    parser.add_argument('-j', '--jobs', type=int, default=1, help="""Specifies
            the number of jobs (recipes) to run simultaneously.""")
    parser.add_argument('-n', '--dry-run', action='store_true', help="""Print
            status messages, but do not run recipes""")
    parser.add_argument('-s', '--silent', action='store_true', help="""Don't
            print the commands that are executed""")
    parser.add_argument('-u', '--pretend-up-to-date', metavar='FILE',
            action='append', default=[],
            help="""Do not rebuild FILE or its dependencies (unless they are
            also depended on by other targets) even if out of date, but make
            sure that future invocations of Produce will still treat them as
            out of date by increasing the modification times of their changed
            dependencies as necessary.""")
    parser.add_argument('target', nargs='*', help="""The target(s) to produce
            - if omitted, default target from Producefile is used""")
    return parser.parse_args(args)

### PRODUCEFILE PARSING

SECTIONHEAD_PATTERN = re.compile(r'^\[(.*)\]\s*$')
AVPAIR_PATTERN = re.compile(r'^(\S+?)\s*=\s*(.*)$', re.DOTALL)
VALUECONT_PATTERN = re.compile(r'^(\s)(.*)$', re.DOTALL)
COMMENT_PATTERN = re.compile(r'^\s*#.*$')
BLANKLINE_PATTERN = re.compile(r'^\s*$')

def parse_inifile(f):
    in_block = False
    current_name = None
    current_avpairs = None
    def current_section():
        return current_name, [(a, v.strip()) for (a, v) in current_avpairs]
    def extend_current_value(string):
        a, v = current_avpairs.pop()
        current_avpairs.append((a, v + string))
    for lineno, line in enumerate(f, start=1):
        match = SECTIONHEAD_PATTERN.match(line)
        if match:
            if in_block:
                yield current_section()
            in_block = True
            current_name = match.group(1)
            current_avpairs = []
            continue
        match = COMMENT_PATTERN.match(line)
        if match:
            continue
        if in_block:
            match = AVPAIR_PATTERN.match(line)
            if match:
                current_avpairs.append((match.group(1), match.group(2)))
                current_valuecont_pattern = VALUECONT_PATTERN
                continue
            if current_avpairs:
                match = current_valuecont_pattern.match(line)
                if match:
                    # Modify pattern for value continutation lines to cut off 
                    # only as much whitespace as the first continutation line
                    # starts with:
                    current_valuecont_pattern = re.compile(
                            r'^(' + match.group(1) + r')(.*)$', re.DOTALL)
                    extend_current_value(match.group(2))
                    continue
        match = BLANKLINE_PATTERN.match(line)
        if match:
            if current_avpairs:
                extend_current_value(os.linesep) 
            continue
        raise ProduceError('invalid line {} in Producefile'.format(lineno))
    if in_block:
        yield current_section()

def interpret_sections(sections):
    raw_globes = {}
    rules = []
    at_beginning = True
    for name, avpairs in sections:
        if at_beginning:
            at_beginning = False
            if name == '':
                raw_globes = OrderedDict(avpairs)
                continue
        if name == '':
            raise ProduceError('The global section (headed []) is only allowed'
                    ' at the beginning of the Producefile.')
        else:
            rules.append((section_name_to_regex(name), OrderedDict(avpairs)))
    return raw_globes, rules

### PATTERN MATCHING AND INTERPOLATION

def interpolate(string, varz, ignore_undefined=False, keep_escaped=False):
    debug(3, 'interpolate called with varz: %s', {k: v for (k, v) in
            varz.items() if k != '__builtins__'})
    original_string = string
    result = ''
    while string:
        if string.startswith('%%'):
            if keep_escaped:
                result += '%%'
            else:
                result += '%'
            string = string[2:]
        elif string.startswith('%{'):
            # HACK: find the } that terminates the expansion by calling eval on
            # possible expressions of increasing length until we find one that
            # doesn't raise a syntax error. FIXME: things will still blow up if
            # the Python expression contains a comment that contains a }.
            start = 2
            error = None
            while '}' in string[start:]:
                debug(3, 'looking for } in %s, starting at %s', string, start)
                index = string.index('}', start)
                # Add parentheses so users don't need to add them for sequence
                # expressions:
                expression = '(' + string[2:index] + ')'
                try:
                    value = eval(expression, varz)
                    debug(3, 'interpolating %s into %s', expression, value)
                    result += value_to_string(value)
                    string = string[index + 1:]
                    break
                except SyntaxError as e:
                    error = e
                except NameError as e:
                    if ignore_undefined:
                        result += string[:index + 1]
                        string = string[index + 1:]
                        break
                    else:
                        raise ProduceError(('name error in expression "{}" ' +
                                'in string "{}": {}').format(expression,
                                original_string, e))
                start = index + 1
            else:
                if error:
                    raise ProduceError(('{} evaluating expression "{}" in ' +
                            'pattern "{}": {}').format(error.__class__.__name__,
                            expression, original_string, error))
                else:
                    raise ProduceError(
                            'could not parse expression in string {}'.format(
                            original_string))
        elif string.startswith('%'):
            raise ProduceError('% must be followed by % or variable name in ' +
                    'curly braces in pattern {}'.format(original_string))
        else:
            result += string[:1]
            string = string[1:]
    return result

def value_to_string(value):
    if isinstance(value, str):
        return value
    try:
        return shlex_join(value)
    except TypeError:
        return str(value)

def section_name_to_regex(name, globes={}):
    if len(name) > 1 and name.startswith('/') and name.endswith('/'):
        try:
            return re.compile(name[1:-1])
        except Exception as e:
            raise ProduceError('{} in regex {}'.format(e, name))
    else:
        return produce_pattern_to_regex(name, globes)

def produce_pattern_to_regex(pattern, globes={}):
    ip = interpolate(pattern, globes, ignore_undefined=True, keep_escaped=True)
    regex = ''
    while ip:
        if ip.startswith('%%'):
            regex += '%'
            ip = ip[:2]
        # FIXME using the same variable twice in the pattern can be useful but
        # the re library will raise an exception
        elif ip.startswith('%{') and '}' in ip:
            # TODO check that the part between curly braces is a valid
            # Python identifier (or, eventually, expression)
            index = ip.index('}')
            variable = ip[2:index]
            regex += '(?P<' + variable + '>.*)'
            ip = ip[index + 1:]
        else:
            regex += re.escape(ip[:1])
            ip = ip[1:]
    regex += '$' # re.match doesn't enforce reaching the end by itself
    debug(3, 'generated regex: %s', regex)
    return re.compile(regex)

### INSTANTIATED RULES

# An instantiated rule is represented by a dict that corresponds to one rule
# section from the Producefile, with the values already expanded. There are two
# special keys, target (the target as matched by the section header), and type,
# which must be one of file and task and defaults to file.

def create_irule(target, rules, globes):
    # Go through rules until a pattern matches the target:
    for pattern, avdict in rules:
        match = pattern.match(target)
        if match:
            # Dictionary representing the instantiated rule:
            result = {}
            # Dictionary for local variables:
            # TODO is the empty string a good default?
            varz = dict(globes, **match.groupdict(default = ''))
            # Special attribute: target
            result['target'] = target
            varz['target'] = target
            # Process attributes and their values:
            for attribute, value in avdict.items():
                # Remove prefix from attribute to get local variable name:
                loke = attribute.split('.')[-1]
                if loke == 'target':
                    raise ProduceError('cannot overwrite "target" attribute ' +
                            'in rule for {}'.format(target))
                # Do expansions in value:
                iv = interpolate(value, varz)
                # Attribute retains prefix:
                result[attribute] = iv
                # Local variable does not retain prefix:
                varz[loke] = iv
                # If there is a condition and it isn't met, we stop processing
                # attributes so they don't raise errors:
                if attribute == 'cond' and not ast.literal_eval(iv):
                    break
            # If there is a condition and it isn't met, go to the next rule:
            if 'cond' in result and not ast.literal_eval(result['cond']):
                debug(3, 'condition %s failed, trying next rule',
                        result['cond'])
                continue
            debug(3, 'instantiated rule for target %s has varz: %s', target,
                    {k: v for (k, v) in varz.items() if k != '__builtins__'})
            result['type'] = result.get('type', 'file')
            debug(3, 'target type: %s', result['type'])
            if result['type'] not in ('file', 'task'):
                raise ProduceError('unknown type {} for target {}'.format(
                    target_type, target))
            return result
        else:
            debug(3, 'pattern %s did not match, trying next rule', pattern)
    if os.path.exists(target):
        # Although there is no rule to make the target, the target is a file
        # that exists, so we can use it as an ingredient.
        return {'target': target, 'type': 'file'}
    raise ProduceError('no rule to produce {}'.format(target))

def list_ddeps(irule):
    result = []
    for key, value in irule.items():
        if key.startswith('dep.'):
            result.append(value)
        elif key == 'deps':
            result.extend(shlex.split(value))
        elif key == 'depfile':
            try:
                result.extend(read_depfile(value))
            except IOError as e:
                raise ProduceError('cannot read depfile {} for target {}' \
                        .format(value, irule['target']), e)
    return result

def read_depfile(filename):
    with open(filename) as f:
        return list(map(str.strip, f))

### PRODUCTION

class Producer(Thread):

    """
    A producer is an "actor" in charge of producing a single target. It is a
    thread: you create it, you start it, and when it's done, it will put itself
    onto the queue that you passed to the constructor. From there you can get
    it and call get_result to get the result.
    """

    def __init__(self, production, target, depth=0, queue=None):
        Thread.__init__(self)
        self.production = production
        self.target = target
        self.depth = depth
        self.queue = queue
        self.exception = None

    def run(self):
        try:
            self.result = self.build_if_required()
        # Catch exceptions and store them for when the calling thread tries to
        # get the result. We could store the stacktrace, too, but currently
        # it's not needed.
        except (Exception, KeyboardInterrupt) as e:
            self.exception = e
        # Send message that we're done:
        if self.queue is not None:
            self.queue.put(self)

    def build_if_required(self):
        """
        (Re)builds the target of this producer unless it is already up to date.
        This method is mainly for up-to-dateness bookkeeping and co-ordinating
        with other Producers that are running concurrently. The core work is
        delegated to _build.
        """
        # We lock all side outputs.
        outputs = set(self.production.target_outputs[self.target])
        lockables = set(outputs)
        # We also lock the target iff we have a recipe for it:
        if self.production.target_irule[self.target].get('recipe'):
            lockables.add(self.target)
        else:
            lockables.discard(self.target) # in case it's in outputs
        # Sort lockables to prevent deadlocks:
        lockables = sorted(lockables)
        with ExitStack() as stack: # TODO ExitStack only in 3.3 and above, can we use something else?
            # Acquire locks. Release is automatic on leaving the context of
            # stack.
            for lockable in lockables:
                debug(3, 'locking %s', lockable)
                with self.production.fields_lock:
                    lock = self.production.target_lock[lockable]
                stack.enter_context(lock)
                debug(3, 'locked %s', lockable)
            # Check if the target still needs to be built:
            with self.production.fields_lock:
                # If the target is up-to-date and non-missing, we're done:
                if not (self.production.is_out_of_date(self.target) or \
                        self.production.is_missing(self.target)):
                    return False
                # If building this target encountered an exception before, we
                # also raise it, in case this thread is joined earlier than the
                # one which originally raised it:
                if self.target in self.production.target_exception:
                    raise self.production.target_exception[self.target]
            # Build the target:
            try:
                self._build()
            except Exception as e:
                with self.production.fields_lock:
                    self.production.target_exception[self.target] = e
                raise e
            # Mark all outputs as now up-to-date and non-missing:
            with self.production.fields_lock:
                for output in outputs | set((self.target,)):
                    self.production.out_of_date_targets.discard(output)
                    self.production.missing_targets.discard(output)
        return True

    def _build(self):
        """
        If it has been decided that a target is out of date, this is the
        method that (re)builds it, unless we use the -u feature and pretend the
        target is up to date. As a prerequisite, the method first makes sure
        all direct dependencies are up to date by recursively calling _produce.
        """
        if self.target in self.production.pretend_up_to_date:
            return # TODO need to raise error somewhere if target is a file and doesn't exist
        # Make sure dependencies are up to date:
        self.production._produce(self.production.target_ddeps[self.target],
                depth=self.depth + 1)
        # Run recipe. Semaphore limits number of parallel recipes:
        with self.production.build_sema:
            if self.production.killed:
                raise ProduceError('recipe killed: {}'.format(recipe))
            self._run_recipe()

    def _run_recipe(self):
        """
        This is where it really happens: the recipe for a target is run, and we
        tell the user about it.
        """
        irule = self.production.target_irule[self.target]
        if not 'recipe' in irule:
            return
        target = irule['target']
        if irule['type'] == 'task':
            status_info('running task', target, self.depth)
        else:
            if os.path.exists(target):
                status_info('rebuilding file', target, self.depth)
            else:
                status_info('building file', target, self.depth)
        recipe = irule['recipe']
        executable = irule.get('shell', 'bash')
        if recipe.startswith('\n'):
            recipe = recipe[1:]
        if debug_level >= 1:
            print(recipe)
        if not self.production.dry_run:
            # Remove old backup files, if any:
            if irule['type'] == 'file':
                backup_name = target + '~'
                remove_if_exists(backup_name)
            for output in self.production.target_outputs[target]:
                backup_name = output + '~'
                remove_if_exists(backup_name)
            success = False
            try:
                with NamedTemporaryFile(mode='w') as recipefile:
                    recipefile.write(recipe)
                    recipefile.flush()
                    with self.production.fields_lock:
                        # Start running the recipe:
                        process = subprocess.Popen([executable,
                            recipefile.name], stdin=sys.stdin)
                    # Wait for recipe to finish:
                    exit = None
                    while exit is None:
                        try:
                            exit = process.wait(timeout=0.05)
                        except subprocess.TimeoutExpired:
                            if self.production.killed:
                                process.kill()
                                raise ProduceError('recipe killed: {}'.format(
                                        recipe))
                    debug(3, 'Exit code: %s', exit)
                if exit != 0:
                    raise ProduceError('recipe failed: {}'.format(recipe))
                success = True
                # TODO check here if file was created for file rules, raise
                # exception if not?
            finally:
                if not success:
                    if irule['type'] == 'file':
                        backup_name = target + '~'
                        debug(2, 'renaming %s to %s', target, backup_name)
                        rename_if_exists(target, backup_name)
                    for output in self.production.target_outputs[target]:
                        backup_name = output + '~'
                        debug(2, 'renaming %s to %s', output, backup_name)
                        rename_if_exists(output, backup_name)
                    status_error('incomplete', target, self.depth)
        status_info('complete', target, self.depth)
    
    def get_result(self):
        """
        Returns True if the target was (re)built, False if the target was
        already update. Raises a ProduceError if something went wrong.
        """
        if self.exception:
            raise self.exception
        return self.result

class Production:

    """
    An object of this class represents one run of produce. The methods are the
    building blocks of the algorithm, the attributes represent options and
    state that changes during the production process.
    """

    def __init__(self, targets, rules, globes, dry_run, always_build, jobs,
            pretend_up_to_date, silent):
        # General properties, passed as arguments to constructor:
        self.rules = rules # maps patterns to key-uninstantiated value maps ("rules")
        self.globes = globes
        self.dry_run = dry_run
        self.always_build = always_build
        self.pretend_up_to_date = pretend_up_to_date
        self.silent = silent
        # Create locks for the building phase:
        self.fields_lock = RLock()
        self.target_lock = defaultdict(RLock)
        debug(3, 'using up to %s threads', jobs)
        self.build_sema = BoundedSemaphore(jobs)
        # Initialize fields to store information about individual targets:
        self.targets = []
        self.target_outputs = {}
        self.target_irule = {} # maps targets to key-instantiated value maps ("instantiated rules")
        self.target_ddeps = {} # direct dependencies
        self.target_time = {}
        self.out_of_date_targets = set()
        self.missing_targets = set()
        # If a recipe for a target fails, store the exception for the benefit
        # of other threads that need the same target. Instead of trying to
        # build it again, they can just raise the existing exception.
        self.target_exception = {}
        # Maps a dependent to a changed direct dependency that causes it to be
        # out of date:
        self.changed_ddeps = {}
        # Abort flag:
        self.killed = False

    def add_target(self, target, beam):
        # Catch cyclic dependencies:
        if target in beam:
            raise ProduceError('cyclic dependency: {}'.format(' -> '.join(
                    beam + [target])))
        # Stop on encountering a target for the second time:
        if target in self.targets:
            return
        # Make instantiated rule:
        irule = create_irule(target, self.rules, self.globes)
        # Store it directly (we need it for spotting harmless soft cycles):
        self.target_irule[target] = irule
        # Determine outputs:
        if 'outputs' in irule:
            outputs = shlex.split(irule['outputs'])
        else:
            outputs = []
        # Catch "soft cyclic dependencies". We don't want to allow a target to
        # depend on a rule that will produce it as a side output: the target
        # would be built twice by the same production, which is crazy and
        # probably indicates a bug. There's an exception though: if the target
        # has an empty recipe, it may depend on a target that produces the
        # first target as a side output. This is useful because it allows
        # dependencies on side outputs: they can then declare which target to
        # produce to get the side output.
        for output in outputs:
            if output in beam and self.target_irule[output].get('recipe'):
                raise ProduceError(
                        'cyclic dependency: {}, which has {} as output'.format(
                        ' -> '.join(beam + [target]), output))
        # The first direct dependency is the depfile, if any:
        if 'depfile' in irule:
            ddep = irule['depfile']
            # Add it recursively:
            self.add_target(ddep, beam + [target])
            # Make sure it's up to date:
            Producer(self, ddep).build_if_required() # TODO do this asynchronously like for normal targets
        # Determine other dependencies and add them recursively:
        ddeps = list_ddeps(irule)
        debug(2, '%s <- %s', target, ddeps)
        for ddep in ddeps:
            self.add_target(ddep, beam + [target])
        if ddeps:
            max_ddep_time = max([self.target_time[ddep] for ddep in ddeps])
        else:
            max_ddep_time = 0
        # Determine type, existence and time of target:
        target_type = irule['type']
        if target_type == 'task':
            missing = False
            time = 0
            debug(3, '%s is a task', target)
        else:
            missing = not os.path.exists(target)
            if missing:
                time = max_ddep_time
            else:
                time = mtime(target, 0)
                debug(3, '%s time: %s', target, time)
        debug(3, '%s missing? %s', target, missing)
        # Determine whether target is out of date, based on information about
        # dependencies, all of which are at this point guaranteed to have been
        # added:
        out_of_date = self._check_if_out_of_date(target, target_type, ddeps,
                time)
        # If we did not detect the target to be out of date but we remember it
        # is, caused by some specific newer direct dependency, we need to
        # rectify the situation by touching that dependency, thereby making it
        # out of date, and detectably so by future Produce invocations. This is
        # relevant in the second pass of adding dependencies, after the build
        # process, when we preserve ignored out-of-date statuses. Note that
        # touching a dependency does not risk introducing a new inconsistency
        # because if the dependency were out of date, we would have detected
        # this target to be out of date, too. Note also that if there is no
        # direct newer file dependency but we still remember the target as out
        # of date, we are guaranteed to detect this in a future invocation in
        # some other way - either via an intermediate dependency or because
        # there is a direct task dependency.
        if not out_of_date and target in self.changed_ddeps:
            touch(self.changed_ddeps[target], now() + 1)
        # Store information about this target:
        self.targets.append(target)
        self.target_outputs[target] = outputs
        self.target_ddeps[target] = ddeps
        self.target_time[target] = time
        if out_of_date:
            self.out_of_date_targets.add(target)
        if missing:
            self.missing_targets.add(target)

    def _check_if_out_of_date(self, target, target_type, ddeps, time):
        if target_type == 'task':
            debug(2, '%s is out of date because it is a task', target)
            return True
        for ddep in ddeps:
            if ddep not in self.pretend_up_to_date and \
                    self.is_out_of_date(ddep):
                debug(2, '%s is out of date because its direct dependency %s '
                        'is out of date', target, ddep)
                return True
        for ddep in ddeps:
            if self.target_time[ddep] > time:
                debug(2, '%s is out of date because its direct dependency %s '
                        'changed', target, ddep)
                # Remember which changed ddep caused this target to be out of
                # date - relevant for pretend mode:
                self.changed_ddeps[target] = ddep
                return True
        if self.always_build:
            debug(2, '%s is out of date because we are in "always build" mode',
                    target)
            return True
        return False

    def is_out_of_date(self, target):
        return target in self.out_of_date_targets

    def is_missing(self, target):
        return target in self.missing_targets

    def _produce(self, targets, depth=0):
        """
        Internally used method for producing a list of targets. Returns True if
        something needed to be done, False if all targets were already up to
        date.
        """
        # Build requested targets in parallel:
        producers = []
        q = queue.Queue()
        for target in targets:
            producer = Producer(self, target, depth=depth, queue=q)
            producer.start()
            producers.append(producer)
        # Let all producers finish. If one raises an exception, we induce
        # controlled shutdown in (kill) all others.
        exception = None
        all_up_to_date = True
        for _ in targets:
            if self.killed:
                q.get()
            else:
                try:
                    all_up_to_date = not q.get().get_result() and \
                            all_up_to_date
                except (Exception, KeyboardInterrupt) as e:
                    exception = e
                    self.killed = True
                    debug(3, 'killed production because of %s', e)
        # Propagate exception, if any:
        if exception:
            raise exception
        # Return whether anything was done:
        return not all_up_to_date

    def produce(self, targets):
        """
        After adding all targets, call this method to produce them. High-level
        method responsible for error handling and clean aborts.
        """
        def handle_signal(signum, frame):
            raise ProduceError('received signal')
        old_handler_term = signal.signal(signal.SIGTERM, handle_signal)
        old_handler_hup = signal.signal(signal.SIGHUP, handle_signal)
        if old_handler_term is None:
            old_handler_term = signal.SIG_DFL
        if old_handler_hup is None:
            old_handler_hup = signal.SIG_DFL
        try:
            # Phase 1: build dependency graph and gather information
            # TODO We could probably already start building each target as soon
            # as it's added. Advantages: more elegant, dependency files could
            # be built concurrently.
            for target in targets:
                self.add_target(target, [])
            # Phase 2: build all we need to build
            # TODO Here we'd then not run the producers, just wait for them to
            # finish. How? Via centrally kept queues?
            if not self._produce(targets):
                logging.info('all targets are up to date')
        finally:
            # Phase 3: repeat phase 1 for the pretend targets, touching files
            # in order to preserve out-of-dateness
            self.targets = [] # circumvent add_target's deduplication
            for target in self.pretend_up_to_date:
                self.add_target(target, [])
            signal.signal(signal.SIGTERM, old_handler_term)
            signal.signal(signal.SIGHUP, old_handler_hup)

### MAIN

def produce(args=[]):
    args = process_commandline(args)
    set_up_logging(args.debug)
    try:
        with open(args.file) as f:
           sections = list(parse_inifile(f))
           debug(3, 'parsed sections: %s', sections)
           raw_globes, rules = interpret_sections(sections)
    except IOError as e:
        raise ProduceError('cannot read file {}'.format(args.file), e)
    globes = {}
    for att, val in raw_globes.items():
        globes[att] = interpolate(val, globes)
    # Determine targets:
    targets = args.target
    if not targets:
        if 'default' in globes:
            targets = shlex.split(globes['default'])
        else:
            raise ProduceError('Don\'t know what to produce. Specify a ' +
                    'target on the command line or a default target in ' +
                    'produce.ini.')
    # Execute prelude, with globes providing the name context:
    exec(globes.get('prelude', ''), globes)
    # Produce:
    Production(targets, rules, globes, args.dry_run, args.always_build,
            args.jobs, args.pretend_up_to_date, args.silent).produce(targets)

if __name__ == '__main__':
    try:
        produce(None)
    except ProduceError as e:
        logging.error(e) # FIXME prints only first line of error message???
        sys.exit(1)
    except KeyboardInterrupt as e:
        logging.error('Interrupted')
        sys.exit(1)
